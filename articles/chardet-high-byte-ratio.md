---
title: "文字コード自動検出の「その先」を、バイト列が教えてくれた"
emoji: "🔍"
type: "tech"
topics: ["encoding", "chardet", "typescript", "nodejs"]
published: false
publication_name: "dress_code"
---

# TL;DR

- chardet はマルチバイト系（CJK）には強いが、**シングルバイト系（タイ語・ベトナム語等）は構造的に区別しづらく誤検出しやすい**
- バイト列中の **high byte（0x80 以上）の割合**を見るだけで、タイ語とベトナム語を分離できる
- 候補でデコード → 言語固有文字の有無を検証 → 外れたら元に戻す、という**安全に試行できる設計**がプロダクション投入の鍵

# はじめに

こんにちは、Dress Code でプロダクトエンジニアをしている [ないとー](https://x.com/_bwkw_) です！私たちは [DRESS CODE](https://www.dress-code.com/ja) という多国籍企業向けの業務 OS を開発しています。

このプロダクトには、各国の拠点が業務データを CSV でアップロードする基盤があります。日本、韓国、中国、タイ、ベトナム、インドネシア、マレーシア、フランス、ドイツ...世界中から CSV が飛んできます。問題は、各国のローカルエンコーディングがバラバラだということです。

後続の処理でエンコーディングを意識したくなかったので、アップロードされた CSV のエンコーディングを自動検出して UTF-8 に変換する仕組みが必要でした。

自動検出には [chardet](https://github.com/runk/node-chardet)（Mozilla の自動検出アルゴリズムをベースにしたライブラリ）、変換には [iconv-lite](https://github.com/ashtuchkin/iconv-lite) を採用しました。マルチバイト系はこの組み合わせでうまくいったのですが、シングルバイト系のエンコーディングでは誤検出が多発し、一筋縄ではいきませんでした。

この記事では、chardet の限界に直面し、バイト列の統計的特徴をたった 1 つ見るだけで突破した話を紹介します。同じようにエンコーディング自動検出で苦戦している方の参考になれば幸いです👋

# chardet の守備範囲を知る

chardet は、バイト列からエンコーディングと使用言語を推定する統計ベースの判定器です。

内部では、エンコーディングと言語の組み合わせごとに事前生成した言語モデルを持っていて、シングルバイト系では 2 文字連の頻度分布、マルチバイト系では特定コードポイントの出現分布と状態遷移パターンをモデル化しています。入力バイト列をこれらのモデルに順番に食わせ、各モデルごとに「どれくらいその言語らしい分布になっているか」をスコアリングし、その中でもっともスコアが高かったものを `encoding` として、そのときのスコアを `confidence` として返します。

https://chardet.readthedocs.io/en/latest/how-it-works.html

こうした言語モデルと状態機械のおかげで、**マルチバイトエンコーディング（MBCS）にはかなり強い**です。

EUC-KR や GB18030、Big5 などは、高い confidence で安定して検出できます（少なくとも、自前で用意したテストデータの範囲ではほぼ外しません）。マルチバイトエンコーディングは「特定の範囲のバイトしか出ない」「バイト列のパターンがはっきりしている」といった構造的な特徴が強く、統計モデルが効きやすいのだと思います。

一方で、**シングルバイトエンコーディング（SBCS）はそうはいきません**。

SBCS は 0x00〜0xFF の 256 個のバイトを 1 文字ずつ割り当てる方式で、ISO-8859-1（西欧）、ISO-8859-7（ギリシャ語）、TIS-620（タイ語）、Windows-1258（ベトナム語）などは、同じ 256 スロットを「どの文字を置くか」だけ変えて共有しています。

バイト列だけを見る chardet からすると、これらは構造的によく似ています。ASCII 領域はどれもほぼ同じですし、0x80 以降に出てくる文字も「高位ビットが立ったラテン文字や記号」が中心で、言語ごとの差があまり大きくありません。その結果、「ギリシャ語モデルにもそこそこ合うし、タイ語モデルにもそこそこ合う」といった僅差の勝負になりやすく、実際に自前のテストデータの範囲でも、タイ語（TIS-620）がギリシャ語（ISO-8859-7）に、ベトナム語（Windows-1258）が西欧（ISO-8859-1）に誤検出されるケースが多く見られました。

これは chardet のバグというより、「シングルバイトエンコーディング同士が同じ 1 バイト空間を共有している」という方式そのものの限界に近い挙動です。そのため、「chardet を入れたから終わり」ではなく、「特に 1 バイト系は誤検出を前提に、何かしらガードを足す必要がある」というのが、この時点での結論でした。

# バイト列を眺めてみる

chardet の守備範囲の外にある SBCS をどう区別するか。ここからは、完璧ではないが実用的に十分な判定手法、いわゆる [Heuristic](https://en.wikipedia.org/wiki/Heuristic) を自前で組み立てていきます。

> A heuristic is any approach to problem solving that employs a pragmatic method that is not fully optimized, perfected, or rationalized, but is nevertheless "good enough" as an approximation.

手がかりを探すために、タイ語 CSV とベトナム語 CSV のバイト列の分布を調べてみました。

着目したのは **high byte ratio ＝ バイト列全体のうち 0x80 以上（high byte）が占める割合**です。シングルバイトエンコーディングでは、言語固有の文字がこの領域に配置されているため、言語によって high byte の出現頻度が大きく異なります。

タイ語（TIS-620）では、タイ文字が 0xA1〜0xFB に集中しています。タイ文字はすべて 0x80 以上のバイトにマッピングされているため、テキスト部分は丸ごと high byte（0x80 以上）です。CSV 中で low byte（0x7F 以下）になるのはカンマ・改行・数字といった ASCII 文字だけです。

```
バイト値:  0x00 .................. 0x7F 0x80 .................. 0xFF
           [  ASCII (数字,カンマ)  ]    [ ████████████████████████ ]
                 少ない                      タイ文字がぎっしり
```

一方、ベトナム語（Windows-1258）はラテン文字ベースです。Nguyễn、Trần、Hương のように、文字列の大部分は ASCII 範囲のラテン文字で、声調記号付きの文字（ơ, ư, ă, đ など）だけが 0x80 以上を使います。

```
バイト値:  0x00 .................. 0x7F 0x80 .................. 0xFF
           [ ██████████████████████ ]    [  ░░  ]
               ラテン文字が大部分            声調記号だけ
```

この差は実際にどの程度か。各言語でリアルな CSV を 10,000 行生成し、テキストカラムと数値カラムの比率を変えて計測しました。

| CSV の列構成         | タイ語(TIS-620) | ベトナム語(Windows-1258) | 差   |
| -------------------- | --------------- | ------------------------ | ---- |
| テキスト 3 : 数値 1  | **≒ 77%**       | **≒ 23%**                | 54pt |
| テキスト 5 : 数値 1  | **≒ 83%**       | **≒ 24%**                | 59pt |
| テキスト 10 : 数値 0 | **≒ 90%**       | **≒ 27%**                | 63pt |

:::message
これは模擬データによる計測値なので、実際の CSV の内容（声調記号の密度や列構成）によって数ポイント程度のズレはあり得ます。また、数値カラムが大半を占める極端な CSV では両方とも比率が下がり、この Heuristic だけでは区別できなくなります。その場合は次のセクションで説明するデコード検証が安全装置として機能します。
:::

ベトナム語は子音がすべて ASCII で、high byte になるのは一部の母音（ơ, ư, ă 等）と声調記号の combining mark だけです。そのため、声調記号が多い CSV でも Windows-1258 の high byte ratio は 30% 未満に収まります。一方、TIS-620 は典型的な構成（テキスト 3 : 数値 1）でも約 77% あり、両者の間には常に 50 ポイント以上の差があります。

この差を利用して、閾値を中間にあたる **50%** に設定しました。high byte ratio が 50% を超えていたら「タイ語（TIS-620）候補」、50% 以下なら「ベトナム語（Windows-1258）候補」として扱います。ただし、これはあくまで候補を絞る段階です。最終的な確定は、次のセクションで説明するデコード検証で行います。

# デコード検証で確定する

high byte ratio による振り分けは、あくまで「候補を絞る」段階です。候補が本当に正しいかどうかは、実際にデコードして確認します。

候補のエンコーディングでバイト列をデコードし、以下の 2 つの条件を両方満たした場合だけ採用します。

1. **言語固有文字が 1 文字以上含まれている** — タイ語なら Unicode の Thai ブロック（U+0E00〜U+0E7F）、ベトナム語なら ơ, ư, ă, đ といった固有のラテン拡張文字。正しいエンコーディングでデコードできていれば、これらの文字が必ず現れるはずです。
2. **U+FFFD（文字化け）が増えていない** — 候補でデコードした結果の U+FFFD の数が、元の chardet 結果以下であること。エンコーディングが間違っていると、デコードできないバイト列が U+FFFD に置き換わるため、文字化けの増減で正しさを判断できます。

条件を満たさなければ、候補は棄却して chardet の元の結果に戻ります。つまり、Heuristic が外れても既存の動作を壊しません。この「安全に試行できる」構造が、プロダクション投入のハードルを大きく下げました。

さらに、エンコーディング確定後の UTF-8 変換でも U+FFFD を常時監視し、1 文字でも検出したらエラーで処理を止めるようにしています。正常な業務 CSV に U+FFFD が含まれることは実質ないので、1 文字でもあれば判定ミス確定です。文字化けデータを後続に流すよりも、その場で止めるほうが安全という判断です。

# おわりに

chardet はマルチバイト系（CJK）には強い一方で、シングルバイト系（タイ語・ベトナム語・各種 ISO-8859）は構造的に区別しづらい。これはライブラリのバグではなく、シングルバイトエンコーディングという仕組みの本質的な限界です。

しかし、バイト列を観察すると、言語によって high byte（0x80 以上）の割合が大きく違うことがわかります。この差を利用した閾値判定とデコード検証を組み合わせることで、タイ語・ベトナム語を含む多言語の CSV が文字化けなく通るようになりました。

ライブラリに頼り切るのではなく、**実データのバイト列を眺めてみる**。そこに答えがあるかもしれません。エンコーディング自動検出で同じような壁にぶつかっている方の参考になれば幸いです。
